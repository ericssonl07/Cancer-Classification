{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"]}],"source":["# Training with M1 GPU\n","import tensorflow as tf\n","import os\n","print(tf.config.list_physical_devices('GPU'))\n","os.environ['CUDA_VISIBLE_DEVICES'] = '1'"]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-03-13T12:23:17.683619Z","iopub.status.busy":"2024-03-13T12:23:17.683073Z","iopub.status.idle":"2024-03-13T12:23:17.695943Z","shell.execute_reply":"2024-03-13T12:23:17.694579Z","shell.execute_reply.started":"2024-03-13T12:23:17.683554Z"},"trusted":true},"outputs":[],"source":["# IMPORTING LIBRARIES\n","\n","import matplotlib.pyplot as plt # Plotting\n","from mpl_toolkits.mplot3d import Axes3D # 3D plotting\n","import tensorflow as tf # Tensors and AI utilities\n","import seaborn as sns # Plotting\n","import pandas as pd # Data manipulation\n","import numpy as np # Linear algebra\n","\n","from sklearn.model_selection import train_test_split, RandomizedSearchCV # Cross validation tools\n","from sklearn.tree import plot_tree # Decision tree visualization\n","from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, log_loss # Results metrics and visualization\n","from sklearn.utils import resample # Data balancing\n","\n","from sklearn.ensemble import VotingClassifier as VC # Voting Classifier\n","from sklearn.neighbors import KNeighborsClassifier as KNC # Clustering\n","from sklearn.linear_model import LogisticRegression as LR # Logistic regression\n","from sklearn.tree import DecisionTreeClassifier as DTC # Decision Tree\n","from sklearn.ensemble import RandomForestClassifier as RFC # Random forest\n","from sklearn.ensemble import AdaBoostClassifier as ABC # Adaptive boosting\n","from sklearn.ensemble import GradientBoostingClassifier as GBC # Gradient boosting\n","from xgboost import XGBClassifier as XBC # Extreme gradient boosting\n","\n","from sklearn.base import BaseEstimator, ClassifierMixin\n","\n","from keras.layers import (\n","    Input, # Input layer\n","    Flatten, # Dimension flattening\n","    Dense, # Densely connected layer\n","    LeakyReLU, # Activation\n","    Softmax, # Probability activation (final)\n","    Conv2D, # Convolutional filters\n","    BatchNormalization, # Normalization\n","    Dropout, # Regularization\n","    MaxPool2D # Maximum pooling filters\n",")\n","from keras.utils import (\n","    to_categorical, # One-hot encoding\n","    image_dataset_from_directory, # tf.data.Dataset with inferred classes from folder\n","    plot_model # Keras model visualization\n",")\n","from keras.callbacks import (\n","    ModelCheckpoint, # Model saving\n","    EarlyStopping # Early stopping\n",")\n","from keras.models import (\n","    Model, # Keras API model object\n","    Sequential, # Sequential model object\n","    load_model # Pre-trained model loading\n",")\n","from keras.optimizers.legacy import Adam # Backpropagation\n","\n","from typing import Tuple, List # Misc. typing (for Classifier)\n","\n","# Silencing sklearn warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","warnings.simplefilter(\"ignore\", category = UserWarning)\n","warnings.simplefilter(\"ignore\", category = FutureWarning)\n","warnings.simplefilter(\"ignore\", category = DeprecationWarning)"]},{"cell_type":"markdown","metadata":{},"source":["# Reading Data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-13T12:23:17.698915Z","iopub.status.busy":"2024-03-13T12:23:17.698431Z","iopub.status.idle":"2024-03-13T12:23:17.987311Z","shell.execute_reply":"2024-03-13T12:23:17.985921Z","shell.execute_reply.started":"2024-03-13T12:23:17.698872Z"},"trusted":true},"outputs":[],"source":["data = pd.read_csv('data.csv')\n","data.drop(['id', 'Unnamed: 32'], axis = 1, inplace = True) # Dropping id axis and Unnamed (N/A) axis\n","sns.countplot(data, x = 'diagnosis') # Plotting output class distribution"]},{"cell_type":"markdown","metadata":{},"source":["# Resampling to balance data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-13T12:23:17.989201Z","iopub.status.busy":"2024-03-13T12:23:17.988840Z","iopub.status.idle":"2024-03-13T12:23:18.004782Z","shell.execute_reply":"2024-03-13T12:23:18.003465Z","shell.execute_reply.started":"2024-03-13T12:23:17.989169Z"},"trusted":true},"outputs":[],"source":["malignant = data[data['diagnosis'] == 'M'] # Malignant diagnoses\n","benign = data[data['diagnosis'] == 'B'] # Benign diagnoses\n","resampledMalignant = resample ( # Data resampling to balance\n","    malignant,\n","    replace = True,\n","    n_samples = len(benign),\n","    random_state = 42\n",")\n","\n","data = pd.concat([benign, resampledMalignant]) # Combining resampled data\n","\n","features = data.iloc[:,1:] # Independent variables\n","diagnosis = data.iloc[:,:1].replace({'M': 1, 'B': 0}) # Diagnosis"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-13T12:23:18.007640Z","iopub.status.busy":"2024-03-13T12:23:18.007212Z","iopub.status.idle":"2024-03-13T12:23:18.041894Z","shell.execute_reply":"2024-03-13T12:23:18.040911Z","shell.execute_reply.started":"2024-03-13T12:23:18.007605Z"},"trusted":true},"outputs":[],"source":["features.info()\n","features.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-13T12:23:18.044026Z","iopub.status.busy":"2024-03-13T12:23:18.042976Z","iopub.status.idle":"2024-03-13T12:23:18.053928Z","shell.execute_reply":"2024-03-13T12:23:18.053011Z","shell.execute_reply.started":"2024-03-13T12:23:18.043989Z"},"trusted":true},"outputs":[],"source":["np.unique(diagnosis.values)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["featureNames = data.columns.values[1:]\n","meanFeatures = featureNames[['mean' in feature for feature in featureNames]]\n","seFeatures = featureNames[['se' in feature for feature in featureNames]]\n","worstFeatures = featureNames[['worst' in feature for feature in featureNames]]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def visualiseFeatures(features, name):\n","    numFeatures = len(features)\n","    numRows, numCols = 10, 10\n","    fig, axes = plt.subplots(nrows = numRows, ncols = numCols, figsize = (45, 45))\n","    fig.suptitle(f'Pairwise variable plots ({name})')\n","    for i in range(numFeatures):\n","        for j in range(numFeatures):\n","            xFeature, yFeature = features[j], features[i]\n","            ax = axes[i, j]\n","            sns.scatterplot (\n","                x = data[xFeature], y = data[yFeature],\n","                c = ['r' if x == 1 else 'b' for x in diagnosis.values],\n","                alpha = 0.2, s = 10, ax = ax\n","            )\n","            ax.set_xlabel(xFeature, fontsize=8)\n","            ax.set_ylabel(yFeature, fontsize=8)\n","            ax.tick_params(axis='x', rotation=45, labelsize=8)\n","            ax.tick_params(axis='y', rotation=45, labelsize=8)\n","\n","    plt.tight_layout()\n","    plt.show()\n","    plt.clf()\n","\n","visualiseFeatures(meanFeatures, 'mean features')\n","visualiseFeatures(seFeatures, 'standard error features')\n","visualiseFeatures(worstFeatures, 'worst features')"]},{"cell_type":"markdown","metadata":{},"source":["# Data splitting\n","No data reserved for validation since classical machine learning does not require early stopping for training."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-13T12:23:18.056024Z","iopub.status.busy":"2024-03-13T12:23:18.055040Z","iopub.status.idle":"2024-03-13T12:23:18.077055Z","shell.execute_reply":"2024-03-13T12:23:18.075604Z","shell.execute_reply.started":"2024-03-13T12:23:18.055964Z"},"trusted":true},"outputs":[],"source":["xTrain, xTest, yTrain, yTest = train_test_split ( # Shuffling and splitting data into train and test\n","    features,\n","    diagnosis,\n","    test_size = 0.15,\n","    stratify = diagnosis, # Balanced diagnoses in training and testing data\n","    random_state = 42\n",")\n","\n","#yTrain, yTest = yTrain.values.ravel(), yTest.values.ravel() # Flattening output"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-13T12:23:18.078978Z","iopub.status.busy":"2024-03-13T12:23:18.078606Z","iopub.status.idle":"2024-03-13T12:23:18.091903Z","shell.execute_reply":"2024-03-13T12:23:18.090898Z","shell.execute_reply.started":"2024-03-13T12:23:18.078946Z"},"trusted":true},"outputs":[],"source":["# Evaluation of a model by visualizing the confusion matrix and displaying the f1 score\n","def evaluate(model, modelName):\n","    # Set subplots (1 by 2)\n","    fig, (ax1, ax2) = plt.subplots (\n","        1, 2, \n","        figsize = [12, 4], \n","        dpi = 300, \n","        clear = True\n","    )\n","    # Predict and score data\n","    trainingPrediction = model.predict(xTrain)\n","    testingPrediction = model.predict(xTest)\n","    interpolationF1 = f1_score(yTrain, trainingPrediction)\n","    extrapolationF1 = f1_score(yTest, testingPrediction)\n","    # Overall title\n","    fig.suptitle(f'{modelName} Confusion Matrices')\n","    # Subplot titles\n","    ax1.title.set_text(('Interpolation (f1 = %.4f)' % (interpolationF1)))\n","    ax2.title.set_text(('Extrapolation (f1 = %.4f)' % (extrapolationF1)))\n","    # Confusion matrices\n","    interpolationConfusion = confusion_matrix(yTrain, trainingPrediction)\n","    extrapolationConfusion = confusion_matrix(yTest, testingPrediction)\n","    # Visualizing confusion matrices\n","    sns.heatmap (\n","        interpolationConfusion, annot = True, fmt = 'd',\n","        cmap = 'YlGnBu', ax = ax1, square = True,\n","        xticklabels = ['Benign', 'Malignant'],\n","        yticklabels = ['Benign', 'Malignant']\n","    )\n","    # Label heatmap axes\n","    ax1.set(xlabel = \"True Class\", ylabel = \"Predicted Class\")\n","    sns.heatmap (\n","        extrapolationConfusion, annot = True, fmt = 'd',\n","        cmap = 'YlGnBu', ax = ax2, square = True,\n","        xticklabels = ['Benign', 'Malignant'],\n","        yticklabels = ['Benign', 'Malignant']\n","    )\n","    # Label heatmap axes\n","    ax2.set(xlabel = \"True Class\", ylabel = \"Predicted Class\")\n","    plt.show()\n","    return extrapolationF1"]},{"cell_type":"markdown","metadata":{},"source":["# Model 1: Nearest Neighbors Algorithm (clustering)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-13T12:23:18.093947Z","iopub.status.busy":"2024-03-13T12:23:18.093505Z","iopub.status.idle":"2024-03-13T12:23:19.530100Z","shell.execute_reply":"2024-03-13T12:23:19.528842Z","shell.execute_reply.started":"2024-03-13T12:23:18.093915Z"},"trusted":true},"outputs":[],"source":["# Defining hyperparameters to be searched\n","clusteringParams = {\n","    'n_neighbors': np.arange(5, 55, 5), # (5, 10, ..., 50)\n","    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n","}\n","\n","# Cross-validation of 15 different hyperparameter combinations\n","bestClusteringParams = RandomizedSearchCV (\n","    KNC(),\n","    clusteringParams,\n","    scoring = 'f1',\n","    n_iter = 15,\n","    random_state = 42\n",").fit(xTrain, yTrain).best_params_"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-13T12:23:19.532667Z","iopub.status.busy":"2024-03-13T12:23:19.532201Z","iopub.status.idle":"2024-03-13T12:23:19.539717Z","shell.execute_reply":"2024-03-13T12:23:19.537725Z","shell.execute_reply.started":"2024-03-13T12:23:19.532625Z"},"trusted":true},"outputs":[],"source":["print(bestClusteringParams)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-13T12:23:19.544458Z","iopub.status.busy":"2024-03-13T12:23:19.543969Z","iopub.status.idle":"2024-03-13T12:23:20.519912Z","shell.execute_reply":"2024-03-13T12:23:20.518611Z","shell.execute_reply.started":"2024-03-13T12:23:19.544426Z"},"trusted":true},"outputs":[],"source":["# Initializing the best found model\n","clusteringClassifier = KNC(**bestClusteringParams)\n","\n","clusteringClassifier.fit(xTrain, yTrain)\n","\n","# Plotting the confusion matrices and displaying the f1 score for interpolation and extrapolation\n","clusteringF1 = evaluate(clusteringClassifier, 'Clustering Classifier')"]},{"cell_type":"markdown","metadata":{},"source":["# Model 2: Logistic Regression"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-13T12:23:20.521868Z","iopub.status.busy":"2024-03-13T12:23:20.521372Z","iopub.status.idle":"2024-03-13T12:23:27.944772Z","shell.execute_reply":"2024-03-13T12:23:27.943212Z","shell.execute_reply.started":"2024-03-13T12:23:20.521833Z"},"trusted":true},"outputs":[],"source":["# Repeat the search steps for each model\n","logisticParams = {\n","    'C': np.linspace(0.5, 4.5, 20)\n","}\n","\n","bestLogisticParams = RandomizedSearchCV (\n","    LR(),\n","    logisticParams,\n","    scoring = 'f1',\n","    n_iter = 1000,\n","    random_state = 42\n",").fit(xTrain, yTrain).best_params_\n","\n","print(bestLogisticParams)\n","\n","logisticClassifier = LR(**bestLogisticParams)\n","\n","logisticClassifier.fit(xTrain, yTrain)\n","\n","logisticF1 = evaluate(logisticClassifier, 'Logistic Regression')"]},{"cell_type":"markdown","metadata":{},"source":["# Model 3: Decision Tree Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-13T12:23:27.946688Z","iopub.status.busy":"2024-03-13T12:23:27.946305Z","iopub.status.idle":"2024-03-13T12:23:29.666565Z","shell.execute_reply":"2024-03-13T12:23:29.664950Z","shell.execute_reply.started":"2024-03-13T12:23:27.946656Z"},"trusted":true},"outputs":[],"source":["# Repeat the search steps for each model\n","decisionTreeParams = {\n","    'criterion': ['gini', 'entropy', 'log_loss'],\n","    'max_depth': np.append(np.arange(2, 21), None),\n","    'min_samples_leaf': np.linspace(0.01, 0.3, 50),\n","    'random_state': [42]\n","}\n","\n","bestDecisionTreeParams = RandomizedSearchCV (\n","    DTC(),\n","    decisionTreeParams,\n","    scoring = 'f1',\n","    n_iter = 15,\n","    random_state = 42\n",").fit(xTrain, yTrain).best_params_\n","\n","print(bestDecisionTreeParams)\n","\n","decisionTree = DTC(**bestDecisionTreeParams)\n","\n","decisionTree.fit(xTrain, yTrain)\n","\n","decisionTreeF1 = evaluate(decisionTree, 'Decision Tree')\n","\n","# Visualizing the decision tree\n","plot_tree(decisionTree, filled = True, rounded = True)"]},{"cell_type":"markdown","metadata":{},"source":["# Model 4: Random Forest Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-13T12:23:29.668685Z","iopub.status.busy":"2024-03-13T12:23:29.668270Z","iopub.status.idle":"2024-03-13T12:24:09.742524Z","shell.execute_reply":"2024-03-13T12:24:09.741363Z","shell.execute_reply.started":"2024-03-13T12:23:29.668651Z"},"trusted":true},"outputs":[],"source":["# Repeat the search steps for each model\n","randomForestParams = {\n","    'n_estimators': np.arange(50, 300, 5),\n","    'criterion': ['gini', 'entropy', 'log_loss'],\n","    'max_depth': np.append(np.arange(2, 21), None),\n","    'min_samples_leaf': np.linspace(0.01, 0.3, 50),\n","    'random_state': [42]\n","}\n","\n","bestRandomForestParams = RandomizedSearchCV (\n","    RFC(),\n","    randomForestParams,\n","    scoring = 'f1',\n","    n_iter = 15,\n","    random_state = 42\n",").fit(xTrain, yTrain).best_params_\n","\n","print(bestRandomForestParams)\n","\n","randomForest = RFC(**bestRandomForestParams)\n","\n","randomForest.fit(xTrain, yTrain)\n","\n","randomForestF1 = evaluate(randomForest, 'Random Forest')"]},{"cell_type":"markdown","metadata":{},"source":["# Model 5: AdaBoost Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-13T12:24:09.744897Z","iopub.status.busy":"2024-03-13T12:24:09.744459Z","iopub.status.idle":"2024-03-13T12:24:30.515739Z","shell.execute_reply":"2024-03-13T12:24:30.514335Z","shell.execute_reply.started":"2024-03-13T12:24:09.744859Z"},"trusted":true},"outputs":[],"source":["# Repeat the search steps for each model\n","adaboostParams = {\n","    'n_estimators': np.arange(10, 100, 5),\n","    'learning_rate': np.linspace(0.4, 1.0, 20),\n","    'random_state': [42]\n","}\n","\n","bestAdaboostParams = RandomizedSearchCV (\n","    ABC(),\n","    adaboostParams,\n","    scoring = 'f1',\n","    n_iter = 15,\n","    random_state = 42\n",").fit(xTrain, yTrain).best_params_\n","\n","print(bestAdaboostParams)\n","\n","adaboost = ABC(**bestAdaboostParams)\n","\n","adaboost.fit(xTrain, yTrain)\n","\n","adaboostF1 = evaluate(adaboost, 'Adaboost')"]},{"cell_type":"markdown","metadata":{},"source":["# Model 6: Gradient Boosting Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-13T12:24:30.519652Z","iopub.status.busy":"2024-03-13T12:24:30.518723Z","iopub.status.idle":"2024-03-13T12:25:02.480895Z","shell.execute_reply":"2024-03-13T12:25:02.479662Z","shell.execute_reply.started":"2024-03-13T12:24:30.519620Z"},"trusted":true},"outputs":[],"source":["# Repeat the search steps for each model\n","gradientBoostingParams = {\n","    'loss': ['log_loss', 'exponential'],\n","    'n_estimators': np.arange(50, 200, 5),\n","    'learning_rate': np.linspace(0.4, 1.0, 20),\n","    'min_samples_leaf': np.linspace(0.01, 0.3, 50),\n","    'max_depth': [2, 3, 4, 5],\n","    'random_state': [42]\n","}\n","\n","bestGradientBoostingParams = RandomizedSearchCV (\n","    GBC(),\n","    gradientBoostingParams,\n","    scoring = 'f1',\n","    n_iter = 15,\n","    random_state = 42\n",").fit(xTrain, yTrain).best_params_\n","\n","print(bestGradientBoostingParams)\n","\n","gradient = GBC(**bestGradientBoostingParams)\n","\n","gradient.fit(xTrain, yTrain)\n","\n","gradientF1 = evaluate(gradient, 'Gradient Boosting')"]},{"cell_type":"markdown","metadata":{},"source":["# Model 7: XGBoost Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-13T12:25:02.483041Z","iopub.status.busy":"2024-03-13T12:25:02.482708Z","iopub.status.idle":"2024-03-13T12:25:14.206852Z","shell.execute_reply":"2024-03-13T12:25:14.205939Z","shell.execute_reply.started":"2024-03-13T12:25:02.483013Z"},"trusted":true},"outputs":[],"source":["xgboost = XBC(num_parallel_tree = 185) # Best n_estimators from Random Forest\n","\n","xgboost.fit(xTrain, yTrain)\n","\n","xgboostF1 = evaluate(xgboost, 'XGBoost')"]},{"cell_type":"markdown","metadata":{},"source":["# Summarizing model results"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2024-03-13T12:53:47.791075Z","iopub.status.busy":"2024-03-13T12:53:47.790528Z","iopub.status.idle":"2024-03-13T12:53:48.113390Z","shell.execute_reply":"2024-03-13T12:53:48.112372Z","shell.execute_reply.started":"2024-03-13T12:53:47.791035Z"},"trusted":true},"outputs":[],"source":["# DataFrame summarizing trained model information and results\n","scores = pd.DataFrame(data = {\n","    'model name': ['clustering', 'logistic', 'decision tree', \n","                   'random forest', 'adaboost', 'gradient boosting', \n","                   'xgboost'],\n","    'model': [clusteringClassifier, logisticClassifier, decisionTree, \n","              randomForest, adaboost, gradient, xgboost],\n","    'f1 score': [clusteringF1, logisticF1, decisionTreeF1, \n","                 randomForestF1, adaboostF1, gradientF1, \n","                 xgboostF1],\n","    'importance': [None, logisticClassifier.coef_[0], decisionTree.feature_importances_,\n","                   randomForest.feature_importances_, adaboost.feature_importances_,\n","                   gradient.feature_importances_, xgboost.feature_importances_]\n","})\n","scores.set_index('model name', inplace = True)\n","\n","# Estimating probabilities\n","scores['proba'] = scores['model'].apply(lambda model: model.predict_proba(xTest))\n","# Finding loss scores\n","scores['loss'] = scores['proba'].apply(lambda proba: log_loss(yTest, proba))\n","\n","# Sorting by descending f1 score then ascending loss\n","scores.sort_values(by = ['f1 score', 'loss'], ascending = [False, True], inplace = True)\n","\n","featureNames = data.columns.tolist()[1:]\n","x = scores['importance']\n","\n","# Linear mapping of data to [0, 1]\n","def normalize(x):\n","    if x is None:\n","        return None\n","    x = np.array(x)\n","    return (x - min(x)) / (max(x) - min(x))\n","\n","# Normalizing scores (shared scale of [0, 1] across all models)\n","scores['normalizedImportance'] = scores['importance'].apply(normalize)\n","\n","# Extracting average importance values\n","importances = np.array([x for x in scores['normalizedImportance'].tolist() if x is not None])\n","averageImportances = [np.mean(x) for x in np.transpose(importances)]\n","importanceFrame = pd.DataFrame(data = {\n","    'feature': featureNames,\n","    'importance': averageImportances\n","})\n","# Sorting and bar-plotting average importance values\n","importanceFrame.sort_values(by = 'importance', inplace = True, ascending = False)\n","sns.barplot(x = importanceFrame['importance'], y = importanceFrame['feature'])\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig = plt.figure(figsize = (16, 12))\n","\n","ax = fig.add_subplot(111, projection = '3d', proj_type = 'ortho')\n","\n","ax.scatter (\n","    data['perimeter_mean'], \n","    data['concave points_worst'], \n","    data['area_worst'], \n","    c = ['r' if x == 1 else 'b' for x in diagnosis.values]\n",")\n","\n","ax.set_box_aspect(aspect = None, zoom = 0.8)\n","\n","ax.set_xlabel('Mean Perimeter', loc = 'left', fontsize = 8)\n","ax.set_ylabel('Extreme Concave Point Count', loc = 'bottom', fontsize = 8)\n","ax.set_zlabel('Extreme Area', fontsize = 8)\n","\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Model 8: Soft-Voting Classifier (ensemble)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-13T13:13:54.859021Z","iopub.status.busy":"2024-03-13T13:13:54.858566Z","iopub.status.idle":"2024-03-13T13:14:08.491800Z","shell.execute_reply":"2024-03-13T13:14:08.490391Z","shell.execute_reply.started":"2024-03-13T13:13:54.858988Z"},"trusted":true},"outputs":[],"source":["modelsUsed = scores[np.logical_and(scores['f1 score'] > 0.95, scores['loss'] < 0.15)]['model']\n","voter = VC (\n","    estimators = list(zip(scores.index, modelsUsed.tolist())),\n","    voting = 'soft'\n",")\n","\n","voter.fit(xTrain, yTrain)\n","\n","voterF1 = evaluate(voter, 'Soft-Voting Classifier')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-13T13:14:08.494574Z","iopub.status.busy":"2024-03-13T13:14:08.494157Z","iopub.status.idle":"2024-03-13T13:14:08.545213Z","shell.execute_reply":"2024-03-13T13:14:08.543943Z","shell.execute_reply.started":"2024-03-13T13:14:08.494539Z"},"trusted":true},"outputs":[],"source":["voterProba = voter.predict_proba(xTest)\n","log_loss(yTest, voterProba)"]},{"cell_type":"markdown","metadata":{},"source":["# Model 9: Feedforward Neural Network"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class FNNClassifier:\n","    def __init__(\n","        self,\n","        inputShape,\n","        outputClassCount: int,\n","        denseLayerCount: int,\n","        denseUnits: List[int],\n","        denseDropoutRates: List[float],\n","        trainingSize: float = 0.6,\n","        testingSize: float = 0.2,\n","        validationSize: float = 0.2,\n","        srand: int = 42\n","    ):\n","        self.inputShape = inputShape\n","        self.outputClassCount = outputClassCount\n","        self.denseLayerCount = denseLayerCount\n","        self.denseUnits = denseUnits\n","        self.denseDropoutRates = denseDropoutRates\n","        self.srand = srand\n","        self.layers = []\n","        \n","        assert self.denseLayerCount > 0\n","        assert len(self.denseUnits) == self.denseLayerCount\n","        assert len(self.denseDropoutRates) == self.denseLayerCount\n","\n","        self._construct()\n","\n","    def _construct(self) -> None:\n","        self.layers.append(Input(shape = self.inputShape, name = \"Input\"))\n","        for denseLayer in range(self.denseLayerCount):\n","            self.layers.append (\n","                Dropout (\n","                    rate = self.denseDropoutRates[denseLayer],\n","                    name = f\"Dropout_{denseLayer + 1}\"\n","                ) (\n","                    BatchNormalization (\n","                        name = f\"Batch_Normalization_{denseLayer + 1}\"\n","                    ) (\n","                        LeakyReLU (\n","                            name = f\"Leaky_ReLU_{denseLayer + 1}\"\n","                        ) (\n","                            Dense (\n","                                units = self.denseUnits[denseLayer],\n","                                name = f\"Dense_{denseLayer + 1}\"\n","                            ) (\n","                                self.layers[-1]\n","                            )\n","                        )\n","                    )\n","                )\n","            )\n","        self.layers.append (\n","            Softmax (name = \"Softmax\") (\n","                self.layers[-1]\n","            )\n","        )\n","        self.input = self.layers[0]\n","        self.output = self.layers[-1]\n","        self.classifier = Model (\n","            inputs = [self.input],\n","            outputs =[self.output]\n","        )\n","        return None\n","    pass\n","\n","fnnclassifier = FNNClassifier (\n","    inputShape = (30,),\n","    outputClassCount = 2,\n","    denseLayerCount = 8,\n","    denseUnits = [60, 50, 40, 30, 20, 15, 5, 2],\n","    denseDropoutRates = [0.3, 0.4, 0.35, 0.2, 0.2, 0.2, 0, 0]\n",")\n","\n","# The below code was used to train the classifier. \n","# It has been replaced by code loading a pretrained model saved by the below code.\n","\"\"\"\n","fnnmodel = classifier.classifier\n","\n","fnnmodel.compile(optimizer = Adam(0.0005), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n","\n","fnnmodel.fit (\n","    x = xTrain,\n","    y = to_categorical(yTrain, 2),\n","    epochs = 2000,\n","    callbacks = [\n","        ModelCheckpoint (\n","            \"/Users/ericssonlin/code/Python/Vogelsberger/final/nn.keras\",\n","            monitor = 'val_loss',\n","            save_best_only = True,\n","            save_weights_only = False,\n","            mode = 'min',\n","            verbose = 1\n","        )\n","    ],\n","    validation_data = (xTest, to_categorical(yTest, 2))\n",")\n","\"\"\"\n","\n","fnnclassifier.classifier = load_model('nn.keras')\n","\n","fnnmodel = fnnclassifier.classifier"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Set subplots (1 by 2)\n","fig, (ax1, ax2) = plt.subplots (\n","    1, 2, \n","    figsize = [12, 4], \n","    dpi = 300, \n","    clear = True\n",")\n","# Predict and score data\n","fnnTrainingPrediction = np.argmax(fnnmodel.predict(xTrain), axis = 1)\n","fnnTestingPrediction = np.argmax(fnnmodel.predict(xTest), axis = 1)\n","# Overall title\n","fig.suptitle(f'Neural Network Confusion Matrices')\n","# Subplot titles\n","ax1.title.set_text(('Interpolation (f1 = %.4f)' % (f1_score(yTrain, fnnTrainingPrediction))))\n","ax2.title.set_text(('Extrapolation (f1 = %.4f)' % (f1_score(yTest, fnnTestingPrediction))))\n","# Visualizing confusion matrices\n","sns.heatmap (\n","    confusion_matrix(yTrain, fnnTrainingPrediction), # Matrix\n","    annot = True, # Label values\n","    fmt = 'd', # Integer output\n","    cmap = 'YlGnBu', # Colormap\n","    ax = ax1, # Subplot axis\n","    square = True, # Equal aspect\n","    xticklabels = ['Benign', 'Malignant'], # Ticklabels\n","    yticklabels = ['Benign', 'Malignant']\n",")\n","# Label heatmap axes\n","ax1.set(xlabel = \"True Class\", ylabel = \"Predicted Class\")\n","sns.heatmap (\n","    confusion_matrix(yTest, fnnTestingPrediction), # Matrix\n","    annot = True, # Label values\n","    fmt = 'd', # Integer output\n","    cmap = 'YlGnBu', # Colormap\n","    ax = ax2, # Subplot axis\n","    square = True, # Equal aspect\n","    xticklabels = ['Benign', 'Malignant'], # Ticklabels\n","    yticklabels = ['Benign', 'Malignant']\n",")\n","# Label heatmap axes\n","ax2.set(xlabel = \"True Class\", ylabel = \"Predicted Class\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fnnDF = pd.DataFrame(data = {\n","    'model name': ['feedforward neural network'],\n","    'model': [fnnmodel],\n","    'f1 score': [f1_score(yTest, fnnTestingPrediction)],\n","    'importance': [None]\n","}).set_index('model name')\n","\n","fnnDF['proba'] = fnnDF['model'].apply(lambda model: model.predict(xTest))\n","fnnDF['loss'] = fnnDF['proba'].apply(lambda proba: log_loss(yTest, proba))\n","\n","scores = pd.concat([scores, fnnDF])\n","scores.sort_values(by = ['f1 score', 'loss'], ascending = [False, True], inplace = True)\n","scores"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sns.set_palette('deep')\n","plt.figure(figsize = (10, 6))\n","sns.barplot(data = scores, x = 'f1 score', y = 'model name')\n","plt.title('Balanced F-score (best = 1, worst = 0)')\n","plt.xscale('log'); plt.xlabel('log(f1)'); plt.ylabel('model')\n","plt.show()\n","plt.clf()\n","\n","plt.figure(figsize = (10, 6))\n","sns.barplot(data = scores.sort_values(by = ['loss', 'f1 score'], ascending = True),\n","            x = 'loss', y = 'model name')\n","plt.title('Categorical Cross-Entropy Loss (best = 0, worst = inf)')\n","plt.xscale('log'); plt.xlabel('log(loss)'); plt.ylabel('model')\n","plt.show()\n","plt.clf()"]},{"cell_type":"markdown","metadata":{},"source":["# Model 10: Adaptive-Boosted Neural Network"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class BaseClassifier(BaseEstimator, ClassifierMixin):\n","    def __init__(self, numClasses, optimizer, activation = 'relu'):\n","        self.numClasses = numClasses\n","        self.activation = activation\n","        self.optimizer = optimizer\n","        self.model = None\n","        self.classes_ = None\n","\n","    def fit(self, X, y, sample_weight = None):\n","        self.classes_ = np.unique(y)\n","        numClasses = self.numClasses\n","\n","        self.model = Sequential()\n","        self.model.add(Dense(100, activation = self.activation))\n","        self.model.add(Dense(75, activation = self.activation))\n","        self.model.add(Dense(50, activation = self.activation))\n","        self.model.add(Dense(25, activation = self.activation))\n","        self.model.add(Dense(20, activation = self.activation))\n","        self.model.add(Dense(numClasses, activation = 'softmax'))\n","\n","        self.model.compile(loss = 'categorical_crossentropy', optimizer = self.optimizer, metrics = ['accuracy'])\n","\n","        y_one_hot = np.squeeze(np.eye(numClasses)[y])\n","\n","        self.model.fit(X, y_one_hot, sample_weight = sample_weight, epochs = 20, verbose = 0)\n","\n","        return self\n","\n","    def predict_proba(self, X):\n","        return self.model.predict(X, verbose = 0)\n","\n","    def predict(self, X):\n","        return np.argmax(self.predict_proba(X), axis = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["nnBoost = ABC (\n","    estimator = BaseClassifier (\n","        numClasses = 2, \n","        optimizer = Adam(learning_rate = 0.0005)\n","    )\n",")\n","\n","nnBoost.fit(xTrain, yTrain)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["evaluate(nnBoost, 'Boosted NN')\n","accuracy_score(yTest, nnBoost.predict(xTest))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["base = BaseClassifier(2, Adam(0.0005))\n","base.fit(xTrain, yTrain)\n","evaluate(base, 'Base Classifier (un-boosted)')\n","accuracy_score(yTest, base.predict(xTest))"]},{"cell_type":"markdown","metadata":{},"source":["# Image Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Extracting classes and training data from directory 'lung_image_sets'\n","train, test = image_dataset_from_directory (\n","    directory = '/Users/ericssonlin/code/Python/Vogelsberger/final/lc25000/lung_colon_image_set/lung_image_sets',\n","    labels = 'inferred',\n","    label_mode = 'categorical',\n","    batch_size = 32,\n","    image_size = (384, 384),\n","    shuffle = True,\n","    seed = 42,\n","    validation_split = 0.125,\n","    subset = 'both'\n",")"]},{"cell_type":"markdown","metadata":{},"source":["# Model 1: CNN"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# Classifier class\n","class CNNClassifier:\n","    \"\"\"\n","    The `Classifier` class is made for single-class classification tasks\n","    (those that use categorical cross-entropy as an error function).\n","\n","    # Methods\n","    - `__init__`: constructor\n","    - `loadData`: data loading tool\n","    - `loadSplittedData`: data loading tool\n","    - `preprocess`: loaded data preprocessing (RGB regularization to [0, 1] and one-hot encoding)\n","    - `construct`: layer construction\n","    - `train`: model training\n","    - `evaluate`: model evaluation\n","\n","    # Attributes\n","    - `inputShape`: shape of the input image\n","    - `outputClassCount`: number of output classes\n","    - `convolutionLayerCount`: number of convolution layers\n","    - `convolutionDropoutRates`: dropout layer parameters\n","    - `convolutionFilters`: channel expansion\n","    - `convolutionKernelSizes`: kernel size\n","    - `convolutionStrides`: downscaling\n","    - `poolSizes`: kernel size\n","    - `poolStrides`: downscaling\n","    - `denseLayerCount`: number of dense layers\n","    - `denseUnits`: number of neurons\n","    - `denseDropoutRates`: dropout layer parameters\n","    - `trainingSize`: training proportion\n","    - `testingSize`: testing proportion\n","    - `validationSize`: validation proportion\n","    - `layers`: actual layers (normalization, batch, etc., not included)\n","    - `input`: input layer\n","    - `output`: output layer\n","    - `xTrain, xTest, xValidate, yTrain, yTest, yValidate`: splitted data\n","    - `classifier`: actual classifier (keras Model object)\n","    - `srand`: random seed\n","    \"\"\"\n","    def __init__ (\n","        self,\n","        inputShape: Tuple[int, int, int],\n","        outputClassCount: int,\n","        convolutionLayerCount: int,\n","        convolutionDropoutRates: List[float],\n","        convolutionFilters: List[int],\n","        convolutionKernelSizes: List[int],\n","        convolutionStrides: List[int],\n","        poolSizes: List[int],\n","        poolStrides: List[int],\n","        denseLayerCount: int,\n","        denseUnits: List[int],\n","        denseDropoutRates: List[float],\n","        trainingSize: float = 0.6,\n","        testingSize: float = 0.2,\n","        validationSize: float = 0.2,\n","        srand: int = 42\n","    ):\n","        \"\"\"\n","        Constructor for classifier.\n","        # Parameters\n","        - `inputShape` is the input image shape\n","        - `outputClassCount` is the number of output classes being classified\n","        - `convolutionLayerCount` is the number of convolution layers in the model\n","        - `convolutionDropoutRates` is the dropout rate of each convolution layer\n","        - `convolutionFilters` is the number of filters (channel expansion) in each convolution layer\n","        - `convolutionStrides` gives the downscale factor of each convolution layer\n","        - `poolSizes` gives the sizes of the max-pooling windows\n","        - `poolStrides` gives the downscale factor of each pooling layer\n","        - `denseLayerCount` gives the number of dense layers\n","        - `denseUnits` gives the number of neurons in each dense layer\n","        - `denseDropoutRates` gives the dropout rate for each dense layer\n","        - `trainingSize` gives the proportion of data used for training\n","        - `testingSize` gives the proportion of data used for testing\n","        - `validationSize` gives the proportion of data used for validation\n","        - `srand` gives the randomization seed\n","\n","        # Constraints\n","        - The product of the strides arrays must evenly divide the dimensions of each image\n","        - Each list of layer parameters must match the corresponding layer count filter\n","        - The sum of data proportions must equal 1 (all data used) and there must be no negative numbers\n","        \"\"\"\n","\n","        self.inputShape = inputShape\n","        self.outputClassCount = outputClassCount\n","        self.convolutionLayerCount = convolutionLayerCount\n","        self.convolutionDropoutRates = convolutionDropoutRates\n","        self.convolutionFilters = convolutionFilters\n","        self.convolutionKernelSizes = convolutionKernelSizes\n","        self.convolutionStrides = convolutionStrides\n","        self.poolSizes = poolSizes\n","        self.poolStrides = poolStrides\n","        self.denseLayerCount = denseLayerCount\n","        self.denseUnits = denseUnits\n","        self.denseDropoutRates = denseDropoutRates\n","        self.trainingSize = trainingSize\n","        self.testingSize = testingSize\n","        self.validationSize = validationSize\n","        self.srand = srand\n","        self.layers = []\n","\n","        assert(trainingSize + testingSize + validationSize == 1)\n","        assert(abs(trainingSize) + abs(testingSize) + abs(validationSize) == 1)\n","        assert(self.convolutionLayerCount > 0)\n","        assert(self.denseLayerCount > 0)\n","        assert(len(self.convolutionDropoutRates) == self.convolutionLayerCount)\n","        assert(len(self.convolutionFilters) == self.convolutionLayerCount)\n","        assert(len(self.convolutionKernelSizes) == self.convolutionLayerCount)\n","        assert(len(self.convolutionStrides) == self.convolutionLayerCount)\n","        assert(len(self.poolSizes) == self.convolutionLayerCount)\n","        assert(len(self.poolStrides) == self.convolutionLayerCount)\n","        assert(len(self.denseUnits) == self.denseLayerCount)\n","        assert(len(self.denseDropoutRates) == self.denseLayerCount)\n","        assert (\n","            self.inputShape[0] % (\n","                np.prod(self.convolutionStrides) * np.prod(self.poolStrides)\n","            ) == 0\n","        )\n","        assert (\n","            self.inputShape[1] % (\n","                np.prod(self.convolutionStrides) * np.prod(self.poolStrides)\n","            ) == 0\n","        )\n","\n","        self._construct()\n","\n","    def loadData(self, x, y, requiresResize: bool = True) -> None:\n","        \"\"\"\n","        Load x and y data into the model, splitting into training,\n","        testing, and validation datasets as specified by the `trainingSize`,\n","        `testingSize`, and `validationSize` parameters set in the constructor\n","        \"\"\"\n","        self.xTrain, xRemaining, self.yTrain, yRemaining = train_test_split (\n","            x,\n","            y,\n","            train_size = self.trainingSize,\n","            shuffle = True,\n","            stratify = y,\n","            random_state = self.srand\n","        )\n","        self.xTest, self.xValidate, self.yTest, self.yValidate = train_test_split (\n","            xRemaining,\n","            yRemaining,\n","            train_size = self.testingSize / (1 - self.trainingSize),\n","            shuffle = True,\n","            stratify = yRemaining,\n","            random_state = self.srand\n","        )\n","        self._preprocess(requiresResize)\n","        return None\n","\n","    def loadSplittedData(self, xTrain, yTrain, xTest, yTest, xValidate, yValidate) -> None:\n","        \"\"\"\n","        An alternate way to load already-split data into the model\n","        \"\"\"\n","        self.xTrain, self.yTrain, self.xTest, self.yTest, self.xValidate, self.yValidate = xTrain, yTrain, xTest, yTest, xValidate, yValidate\n","        self._preprocess()\n","        return None\n","\n","    def _preprocess(self, requiresResize: bool = True) -> None:\n","        \"\"\"\n","        Preprocesses all split data (x, y) so that x is a numpy array\n","        with all pixel values between 0 and 1 (normalized) and y is a\n","        numpy array in one-hot-encoded vector format (as expected for\n","        categorical data).\\n\n","        This function is automatically called by the loading functions.\n","        \"\"\"\n","        self.xTrain = np.array(self.xTrain)\n","        self.xTest = np.array(self.xTest)\n","        self.xValidate = np.array(self.xValidate)\n","        if requiresResize:\n","            for image in self.xTrain:\n","                image = tf.image.resize(image, (self.inputShape[0], self.inputShape[1])).numpy()\n","            for image in self.xTest:\n","                image = tf.image.resize(image, (self.inputShape[0], self.inputShape[1])).numpy()\n","            for image in self.xValidate:\n","                image = tf.image.resize(image, (self.inputShape[0], self.inputShape[1])).numpy()\n","        self.yTrain = np.array(to_categorical(self.yTrain, self.outputClassCount))\n","        self.yTest = np.array(to_categorical(self.yTest, self.outputClassCount))\n","        self.yValidate = np.array(to_categorical(self.yValidate, self.outputClassCount))\n","\n","    def _construct(self) -> None:\n","        \"\"\"\n","        Construct layers as instructed by constructor parameters.\\n\n","        Stores the layers in the `self.layers` list, as well as the input\n","        layer in `self.input` and output layer in `self.output`.\\n\n","        Initializes a model stored in `self.classifier`.\n","        \"\"\"\n","        self.layers.append (\n","            Input (shape = self.inputShape, name = \"Input\")\n","        )\n","        for convolutionLayer in range(self.convolutionLayerCount):\n","            self.layers.append (\n","                Dropout (\n","                    rate = self.convolutionDropoutRates[convolutionLayer],\n","                    name = f\"Dropout_{convolutionLayer + 1}\"\n","                ) (\n","                    LeakyReLU (name = f\"LeakyReLU_{convolutionLayer + 1}\") (\n","                        BatchNormalization (name = f\"Batch_Normalization_{convolutionLayer + 1}\") (\n","                            MaxPool2D (\n","                                pool_size = self.poolSizes[convolutionLayer],\n","                                strides = self.poolStrides[convolutionLayer],\n","                                padding = 'same',\n","                                name = f\"Maximum_Pool_{convolutionLayer + 1}\"\n","                            ) (\n","                                Conv2D (\n","                                    filters = self.convolutionFilters[convolutionLayer],\n","                                    kernel_size = self.convolutionKernelSizes[convolutionLayer],\n","                                    strides = self.convolutionStrides[convolutionLayer],\n","                                    padding = 'same',\n","                                    name = f\"Convolution_{convolutionLayer + 1}\"\n","                                ) (\n","                                    self.layers[-1]\n","                                )\n","                            )\n","                        )\n","                    )\n","                )\n","            )\n","        self.layers.append (\n","            Flatten (name = \"Flatten\") (\n","                self.layers[-1]\n","            )\n","        )\n","        for denseLayer in range (self.denseLayerCount):\n","            self.layers.append (\n","                Dropout (\n","                    rate = self.denseDropoutRates[denseLayer],\n","                    name = f\"Dropout_{self.convolutionLayerCount + denseLayer + 1}\"\n","                ) (\n","                    BatchNormalization (name = f\"Batch_Normalization_{self.convolutionLayerCount + denseLayer + 1}\") (\n","                        LeakyReLU (name = f\"Leaky_ReLU_{self.convolutionLayerCount + denseLayer + 1}\") (\n","                            Dense (\n","                                units = self.denseUnits[denseLayer],\n","                                name = f\"Dense_{denseLayer + 1}\"\n","                            ) (\n","                                self.layers[-1]\n","                            )\n","                        )\n","                    )\n","                )\n","            )\n","        self.layers.append (\n","            Softmax (name = f\"Softmax\") (\n","                self.layers[-1]\n","            )\n","        )\n","        self.input = self.layers[0]\n","        self.output = self.layers[-1]\n","        self.classifier = Model (\n","            inputs = [self.input],\n","            outputs = [self.output]\n","        )\n","        return None\n","\n","    def train (\n","        self,\n","        optimizer,\n","        epochs,\n","        callbacks,\n","        loadFile = None\n","    ):\n","        \"\"\"\n","        Train `self.classifier` with loaded data.\\n\n","        Expecting an optimizer (such as keras.optimizers.Adam), the number of epochs\n","        to be passed, callbacks (to be passed to the keras fit function), and\n","        optionally a `.h5` file from which to load pre-trained model weights.\n","        \"\"\"\n","        if loadFile != None:\n","            self.classifier.load_weights(loadFile)\n","        self.classifier.compile (\n","            optimizer = optimizer,\n","            loss = 'categorical_crossentropy',\n","            metrics = [\n","                'accuracy'\n","            ]\n","        )\n","        self.classifier.fit (\n","            x = self.xTrain,\n","            y = self.yTrain,\n","            shuffle = True,\n","            validation_data = (self.xValidate, self.yValidate),\n","            epochs = epochs,\n","            callbacks = callbacks\n","        )\n","\n","    def evaluate(self) -> None:\n","        \"\"\"\n","        Evaluate the trained classifier.\n","        \"\"\"\n","        self.classifier.evaluate (\n","            self.xTest,\n","            self.yTest\n","        )\n","        return None\n","\n","    pass\n","\n","# Classifier instance\n","cnnClassifier = CNNClassifier (\n","    inputShape = (384, 384, 3), # 384x384 images, 3 color channels\n","    outputClassCount = 3, # lung_aca, lung_n, lung_scc\n","    convolutionLayerCount = 6,\n","    convolutionDropoutRates = [0.25, 0.45, 0.2, 0.35, 0.15, 0.2],\n","    convolutionFilters = [16, 32, 32, 64, 16, 32],\n","    convolutionKernelSizes = [1, 5, 2, 7, 3, 5],\n","    convolutionStrides = [1, 2, 2, 6, 1, 2],\n","    poolSizes = [2 for i in range(6)],\n","    poolStrides = [1 for i in range(6)],\n","    denseLayerCount = 2,\n","    denseUnits = [512, 3],\n","    denseDropoutRates = [0.1 for i in range(2)],\n","    trainingSize = 0.75,\n","    testingSize = 0.125,\n","    validationSize = 0.125,\n","    srand = 42\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# The below code was used to train the classifier. \n","# It has been replaced by code loading a pretrained model saved by the below code.\n","\"\"\"\n","cnnModel.compile (\n","    optimizer = Adam(learning_rate = 0.0005),\n","    metrics = ['accuracy'],\n","    loss = 'categorical_crossentropy'\n",")\n","\n","cnnModel.fit (\n","    train,\n","    epochs = 200,\n","    callbacks = [\n","        ModelCheckpoint (\n","            \"/Users/ericssonlin/code/Python/Vogelsberger/final/model4.keras\",\n","            monitor = 'val_loss',\n","            save_best_only = True,\n","            save_weights_only = False,\n","            mode = 'min',\n","            verbose = 1\n","        ),\n","        EarlyStopping (\n","            monitor = 'val_loss',\n","            min_delta = 0.0001,\n","            patience = 5\n","        )\n","    ],\n","    validation_data = test\n",")\n","\n","cnnModel.summary()\n","plot_model(cnnModel, 'model.png', show_shapes = True, dpi = 300)\n","\"\"\"\n","\n","cnnClassifier.classifier = load_model('model4.keras')\n","\n","cnnModel = cnnClassifier.classifier\n","\n","cnnModel.summary()\n","plot_model(cnnModel, 'model.png', show_shapes = True, dpi = 300)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Predict the test set\n","prediction = cnnModel.predict(test)\n","\n","x = []\n","yTrue = []\n","progress = 0\n","names = test.class_names\n","for features, labels in test.take(20):\n","    print(f\"Progress: {progress + 1}\", end = '\\r'); progress += 1\n","    for feature in features:\n","        x.append(feature.numpy())\n","    for label in labels:\n","        yTrue.append(label.numpy())\n","print(\"Converting images to numpy\")\n","x = np.array(x)\n","imagesDisp = x / 255\n","print(\"Done converting images\")\n","yTrue = np.array(yTrue)\n","yTrue = np.array([names[i] for i in np.argmax(yTrue, axis = 1)])\n","\n","def evaluateTrainedModel(modelFile: str):\n","    pretrainedModel = load_model(modelFile)\n","    results = pretrainedModel.evaluate(test)\n","    yPred = pretrainedModel.predict(x)\n","    yPred = np.array([names[i] for i in np.argmax(yPred, axis = 1)])\n","    predictedTruthDisp = yPred\n","    groundTruthDisp = yTrue\n","\n","    fig, axes = plt.subplots(nrows = 4, ncols = 5, figsize = (12, 6))\n","    fig.suptitle(f'Predictions v.s. Ground Truths (model {modelFile})')\n","\n","    for i, ax in enumerate(axes.flatten()):\n","        ax.imshow(imagesDisp[i])\n","        ax.axis('off')\n","        ax.set_title(f'Predicted: {predictedTruthDisp[i]}\\nGround Truth: {groundTruthDisp[i]}')\n","\n","    plt.tight_layout()\n","    plt.show()\n","    plt.clf()\n","    \n","    matrix = confusion_matrix(yTrue, yPred)\n","    sns.heatmap (\n","            matrix,\n","            annot = True,\n","            fmt = 'd',\n","            cmap = 'YlGnBu',\n","            square = True,\n","            xticklabels = ['lung_aca', 'lung_n', 'lung_scc'],\n","            yticklabels = ['lung_aca', 'lung_n', 'lung_scc']\n","    )\n","    plt.title (\n","            'CNN Performance: f1 = %.3f, loss = %.3f' % (\n","                    f1_score(yTrue, yPred, average = \"weighted\"),\n","                    results[0]\n","            )\n","    )\n","    plt.xlabel('Predicted')\n","    plt.ylabel('Actual')\n","    plt.show()\n","    plt.clf()\n","    return f1_score(yTrue, yPred, average = 'weighted'), results[0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["f1, loss = [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]\n","f1[0], loss[0] = evaluateTrainedModel('model0.keras')\n","f1[1], loss[1] = evaluateTrainedModel('model1.keras')\n","f1[2], loss[2] = evaluateTrainedModel('model2.keras')\n","f1[3], loss[3] = evaluateTrainedModel('model3.keras')\n","f1[4], loss[4] = evaluateTrainedModel('model4.keras')"]},{"cell_type":"markdown","metadata":{},"source":["# Model 2: Gradient Boosting on CNN-extracted Features"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["intermediate = Model(cnnClassifier.input, cnnClassifier.layers[8])\n","\n","denseVal = intermediate.predict(train)\n","probaVal = cnnClassifier.classifier.predict(train)\n","\n","denseVal[0], probaVal[0], len(denseVal[0]), len(probaVal[0])\n","\n","dense = np.transpose(denseVal)\n","proba = np.transpose(probaVal)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["yTrue = []\n","for feature, labels in train:\n","    for label in labels:\n","        yTrue.append(label.numpy())\n","yTrue = np.array([names[i] for i in np.argmax(np.array(yTrue), axis = 1)])\n","\n","data = {\n","    'diagnosis': yTrue,\n","    **{i: dense[i] for i in range(512)},\n","    **{i + 512: proba[i] for i in range(3)}\n","}\n","\n","cnnFeatures = pd.DataFrame(data = data)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","cnnFeatures.info()\n","cnnFeatures.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["xTrain, yTrain = cnnFeatures.iloc[:,1:], cnnFeatures.iloc[:,0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["gradientBoostingParams = {\n","    'loss': ['log_loss', 'exponential'],\n","    'n_estimators': np.arange(50, 200, 5),\n","    'learning_rate': np.linspace(0.4, 1.0, 20),\n","    'min_samples_leaf': np.linspace(0.01, 0.3, 50),\n","    'max_depth': [2, 3, 4, 5],\n","    'random_state': [42]\n","}\n","\n","gradient = GBC(**{'random_state': 42, 'n_estimators': 135, 'min_samples_leaf': 0.2763265306122449, 'max_depth': 3, 'loss': 'log_loss', 'learning_rate': 0.7789473684210526})\n","\n","gradient.fit(xTrain, yTrain)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def getFeatures(x):\n","    yActual = []\n","    for feature, labels in x:\n","        for label in labels:\n","            yActual.append(label.numpy())\n","    yActual = np.array([names[i] for i in np.argmax(np.array(yActual), axis = 1)])\n","    densePred = np.transpose(intermediate.predict(x))\n","    probaPred = np.transpose(cnnClassifier.classifier.predict(x))\n","    dfData = {\n","        'diagnosis': yActual,\n","        **{i: densePred[i] for i in range(512)},\n","        **{i + 512: probaPred[i] for i in range(3)}\n","    }\n","    return pd.DataFrame(data = dfData)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trainingFeatures = getFeatures(train)\n","testingFeatures = getFeatures(test)\n","trainingPrediction = gradient.predict(trainingFeatures.iloc[:,1:])\n","testingPrediction = gradient.predict(testingFeatures.iloc[:,1:])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.clf()\n","fig, (ax1, ax2) = plt.subplots (\n","    1, 2, \n","    figsize = [12, 4], \n","    dpi = 300, \n","    clear = True\n",")\n","interpolationF1 = f1_score(trainingFeatures.iloc[:,0], trainingPrediction, average = 'weighted')\n","extrapolationF1 = f1_score(testingFeatures.iloc[:,0], testingPrediction, average = 'weighted')\n","fig.suptitle(f'Extracted-Feature Gradient Boosting Confusion Matrices')\n","ax1.title.set_text(('Interpolation (f1 = %.4f)' % (interpolationF1)))\n","ax2.title.set_text(('Extrapolation (f1 = %.4f)' % (extrapolationF1)))\n","interpolationConfusion = confusion_matrix(trainingFeatures.iloc[:,0], trainingPrediction)\n","extrapolationConfusion = confusion_matrix(testingFeatures.iloc[:,0], testingPrediction)\n","sns.heatmap (\n","    interpolationConfusion, annot = True, fmt = 'd',\n","    cmap = 'YlGnBu', ax = ax1, square = True,\n","    xticklabels = ['Benign', 'Malignant'],\n","    yticklabels = ['Benign', 'Malignant']\n",")\n","ax1.set(xlabel = \"True Class\", ylabel = \"Predicted Class\")\n","sns.heatmap (\n","    extrapolationConfusion, annot = True, fmt = 'd',\n","    cmap = 'YlGnBu', ax = ax2, square = True,\n","    xticklabels = ['Benign', 'Malignant'],\n","    yticklabels = ['Benign', 'Malignant']\n",")\n","ax2.set(xlabel = \"True Class\", ylabel = \"Predicted Class\")\n","plt.show()"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":180,"sourceId":408,"sourceType":"datasetVersion"}],"dockerImageVersionId":30664,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
